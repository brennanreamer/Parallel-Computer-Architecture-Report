@inproceedings{Bell:1999,
	author = {Bell, Gordon},
	title = {The Next Ten Years of Supercomputing},
	booktitle = {Proceedings 14th Supercomputer Conference},
	year = {1999},
	editor = {Hans Werner Meuer},
	address = {Mannheim}
}
@article{Bell:1989,
	author = {Bell, Gordon},
	year = {1989},
	month = {09},
	pages = {1091-1101},
	title = {The Future of High Performance Computers in Science and Engineering},
	volume = {32},
	journal = {Communications of the ACM},
	doi = {10.1145/66451.66457}
}
@techreport{Strohmaier:2005,
	author = {Strohmaier, E.},
	year = {2005},
	title = {20 Years Supercomputer Market Analysis},
	institution = {Lawrence Berkeley National Laboratory},
	number = {LBNL-58442},
	link = {https://escholarship.org/uc/item/4gh5g1cw}
}
@techreport{Bell:2001,
	author = {Bell, Gordon and Gray, Jim},
	title = {High Performance Computing: Crays, Clusters, and Centers. What Next?},
	year = {2001},
	month = {August},
	abstract = {After 50 years of building high performance scientific computers, two major architectures exist: (1) clusters of “Cray-style” vector supercomputers; (2) clusters of scalar uni- and multi-processors. Clusters are in transition from (a) massively parallel computers and clusters running proprietary software to (b) proprietary clusters running standard software, and (c) do-it-yourself Beowulf clusters built from commodity hardware and software. In 2001, only five years after its introduction, Beowulf has mobilized a community around a standard architecture and tools. Beowulf’s economics and sociology are poised to kill off the other two architectural lines – and will likely affect traditional super-computer centers as well. Peer-to-peer and Grid communities provide significant advantages for embarrassingly parallel problems and sharing vast numbers of files. The Computational Grid can federate systems into supercomputers far beyond the power of any current computing center. The centers will become super-data and super-application centers. While these trends make high-performance computing much less expensive and much more accessible, there is a dark side. Clusters perform poorly on applications that require large shared memory. Although there is vibrant computer architecture activity on microprocessors and on high-end cellular architectures, we appear to be entering an era of super-computing mono-culture. Investing in next generation software and hardware supercomputer architecture is essential to improve the efficiency and efficacy of systems. This paper has been submitted for publication to the Communications of the ACM . Copyright may be transferred without further notice and the publisher may then post the accepted version. A version of this article appears at http://research.microsoft.com/pubs/},
	publisher = {Association for Computing Machinery, Inc.},
	url = {https://www.microsoft.com/en-us/research/publication/high-performance-computing-crays-clusters-and-centers-what-next/},
	pages = {7},
	number = {MSR-TR-2001-76},
}
@article{Bakhoda:2009,
	  title={Analyzing CUDA workloads using a detailed GPU simulator},
	  author={Ali Bakhoda and George L. Yuan and Wilson W. L. Fung and Henry Wong and Tor M. Aamodt},
	  journal={2009 IEEE International Symposium on Performance Analysis of Systems and Software},
	  year={2009},
	  pages={163-174}
}
@article{Patterson:2010,
	title={The Trouble With Multicore},
	author={Patterson, David},
	journal={IEEE Spectrum},
	year={2010},
	month={07}
}
@inproceedings{Trefethen:1998,
	title={Predictions for Scientific Computing Fifty Years From Now},
	author={Trefethen, Lloyd},
	institution={Oxford University Computing Laboratory},
	year={1998},
	month={06},
	day={17},
	booktitle={Numerical Analysis and Computers - 50 Years of Progress},
	link={https://people.maths.ox.ac.uk/trefethen/future.pdf}
}
@article{Dabroski:2009,
	title={Reliability in grid computing systems},
	author={Dabrowski, Christopher},
	doi={10.1002/cpe.1410},
	journal={Wiley InterScience},
	year={2009},
	address={National Institute of Standards and Technology, 100 Bureau Drive, Stop 8970,
	Gaithersburg, MD 20899-8970, U.S.A.}
}
@article{Merolla:2014,
	title={Artificial brains. A million spiking-neuron integrated circuit with a scalable communication network and interface},
	author={Merolla, Paul},
	doi={10.1126/science.1254642},
	volume={345,6197},
	year={2014},
	month={08},
	day={07},
	journal={Science}
}
@Article{Kelechi:2020,
	AUTHOR = {Kelechi, Anabi Hilary and Alsharif, Mohammed H. and Bameyi, Okpe Jonah and Ezra, Paul Joan and Joseph, Iorshase Kator and Atayero, Aaron-Anthony and Geem, Zong 		Woo and Hong, Junhee},
	TITLE = {Artificial Intelligence: An Energy Efficiency Tool for Enhanced High performance computing},
	JOURNAL = {Symmetry},
	VOLUME = {12},
	YEAR = {2020},
	NUMBER = {6},
	ARTICLE-NUMBER = {1029},
	URL = {https://www.mdpi.com/2073-8994/12/6/1029},
	ISSN = {2073-8994},
	ABSTRACT = {Power-consuming entities such as high performance computing (HPC) sites and large data centers are growing with the advance in information technology. In business, HPC is used to enhance the product delivery time, reduce the production cost, and decrease the time it takes to develop a new product. Today&rsquo;s high level of computing power from supercomputers comes at the expense of consuming large amounts of electric power. It is necessary to consider reducing the energy required by the computing systems and the resources needed to operate these computing systems to minimize the energy utilized by HPC entities. The database could improve system energy efficiency by sampling all the components&rsquo; power consumption at regular intervals and the information contained in a database. The information stored in the database will serve as input data for energy-efficiency optimization. More so, device workload information and different usage metrics are stored in the database. There has been strong momentum in the area of artificial intelligence (AI) as a tool for optimizing and processing automation by leveraging on already existing information. This paper discusses ideas for improving energy efficiency for HPC using AI.},
	DOI = {10.3390/sym12061029}
}
@ARTICLE{Bhattacharya:2019,
	 AUTHOR={Bhattacharya, Tanmoy and Brettin, Thomas and Doroshow, James H. and Evrard, Yvonne A. and Greenspan, Emily J. and Gryshuk, Amy L. and Hoang, Thuc T. and Lauzon, Carolyn B. Vea and Nissley, Dwight and Penberthy, Lynne and Stahlberg, Eric and Stevens, Rick and Streitz, Fred and Tourassi, Georgia and Xia, Fangfang and Zaki, George},   
	TITLE={AI Meets Exascale Computing: Advancing Cancer Research With Large-Scale High Performance Computing},      	
	JOURNAL={Frontiers in Oncology},      
	VOLUME={9},           
	YEAR={2019},       
	URL={https://www.frontiersin.org/articles/10.3389/fonc.2019.00984},       
	DOI={10.3389/fonc.2019.00984},      
	ISSN={2234-943X},   
	ABSTRACT={The application of data science in cancer research has been boosted by major advances in three primary areas: (1) Data: diversity, amount, and availability of biomedical data; (2) Advances in Artificial Intelligence (AI) and Machine Learning (ML) algorithms that enable learning from complex, large-scale data; and (3) Advances in computer architectures allowing unprecedented acceleration of simulation and machine learning algorithms. These advances help build in silico ML models that can provide transformative insights from data including: molecular dynamics simulations, next-generation sequencing, omics, imaging, and unstructured clinical text documents. Unique challenges persist, however, in building ML models related to cancer, including: (1) access, sharing, labeling, and integration of multimodal and multi-institutional data across different cancer types; (2) developing AI models for cancer research capable of scaling on next generation high performance computers; and (3) assessing robustness and reliability in the AI models. In this paper, we review the National Cancer Institute (NCI) -Department of Energy (DOE) collaboration, Joint Design of Advanced Computing Solutions for Cancer (JDACS4C), a multi-institution collaborative effort focused on advancing computing and data technologies to accelerate cancer research on three levels: molecular, cellular, and population. This collaboration integrates various types of generated data, pre-exascale compute resources, and advances in ML models to increase understanding of basic cancer biology, identify promising new treatment options, predict outcomes, and eventually prescribe specialized treatments for patients with cancer.}
}
@INPROCEEDINGS{Yang:2008,
	  author={Yang, Zhiyi and Zhu, Yating and Pu, Yong},
	  booktitle={2008 International Conference on Computer Science and Software Engineering}, 
	  title={Parallel Image Processing Based on CUDA}, 
	  year={2008},
	  volume={3},
	  number={},
	  pages={198-201},
	  doi={10.1109/CSSE.2008.1448},
	  link={https://ieeexplore.ieee.org/abstract/document/4722322}
}
@article{Ghorpade:2012,
	  author    = {Jayshree Ghorpade and
	               Jitendra Parande and
	               Madhura Kulkarni and
	               Amit Bawaskar},
	  title     = {{GPGPU} Processing in {CUDA} Architecture},
	  journal   = {CoRR},
	  volume    = {abs/1202.4347},
	  year      = {2012},
	  url       = {http://arxiv.org/abs/1202.4347},
	  eprinttype = {arXiv},
	  eprint    = {1202.4347},
	  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
	  biburl    = {https://dblp.org/rec/journals/corr/abs-1202-4347.bib},
	  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Donno:2010,
	author = {De Donno, Danilo and Esposito, Alessandra and Tarricone, Luciano and Catarinucci, Luca},
	year = {2010},
	month = {07},
	pages = {116 - 122},
	title = {Introduction to GPU computing and CUDA programming: a case study on FDTD},
	volume = {52},
	journal = {Antennas and Propagation Magazine, IEEE},
	doi = {10.1109/MAP.2010.5586593}
}
@ARTICLE{Mielikainen:2012,
	  author={Mielikainen, Jarno and Huang, Bormin and Huang, Hung-Lung Allen and Goldberg, Mitchell D.},
	  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
	  title={Improved GPU/CUDA Based Parallel Weather and Research Forecast (WRF) Single Moment 5-Class (WSM5) Cloud Microphysics}, 
	  year={2012},
	  volume={5},
	  number={4},
	  pages={1256-1265},
	  doi={10.1109/JSTARS.2012.2188780}
  }
  @article{Lee:2012,
	  author={Lee D, Dinov},
	  title={CUDA optimization strategies for compute- and memory-bound neuroimaging algorithms},
	  journal={Comput Methods Programs Biomed},
	  year={2012},
	  month={06},
	  doi={10.1016/j.cmpb.2010.10.013},
	  pages={175-187},
	  volume={106(3)}
}
@article{Chakrabarti:2012,
	title = {CUDA: Compiling and optimizing for a GPU platform},
	journal = {Procedia Computer Science},
	volume = {9},
	pages = {1910-1919},
	year = {2012},
	note = {Proceedings of the International Conference on Computational Science, ICCS 2012},
	issn = {1877-0509},
	doi = {https://doi.org/10.1016/j.procs.2012.04.209},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050912003304},
	author = {Gautam Chakrabarti and Vinod Grover and Bastiaan Aarts and Xiangyun Kong and Manjunath Kudlur and Yuan Lin and Jaydeep Marathe and Mike Murphy and Jian-Zhong Wang},
	keywords = {CUDA, GPGPU, compiler optimizations},
	abstract = {Graphics processor units (GPUs) have evolved to handle throughput oriented workloads where a large number of parallel threads must make progress. Such threads are organized around shared memory making it possible to synchronize and cooperate on shared data. Current GPUs can run tens of thousands of hardware threads and have been optimized for graphics workloads. Several high level languages have been developed to easily program the GPUs for general purpose computing problems. The use of high-level languages introduces the need for highly optimizing compilers that target the parallel GPU device. In this paper, we present our experiences in developing compilation techniques for a high level language called CUDA C. We explain the CUDA architecture and programming model and provide insights into why certain optimizations are important for achieving high performance on a GPU. In addition to classical optimizations, we present optimizations developed specifically for the CUDA architecture. We evaluate these techniques, and present performance results that show significant improvements on hundreds of kernels as well as applications.}
}
	@article{Schuman:2022,
	author={Schuman, C.D. and Kulkarni, S.R. and Parsa, M. and Mitchell, J.P. and Date, P. and Kay, B.},
	title={Opportunities for neuromorphic computing algorithms and applications},
	journal={Nature Computational Science},
	year={2022},
	month={01},
day={31},
	pages={10-19},
	volume={2},
	issue={1},
	doi={10.1038/s43588-021-00184-y}
}

@article{DAngelo:2022,
	doi = {10.1088/2634-4386/ac6b50},
	url = {https://dx.doi.org/10.1088/2634-4386/ac6b50},
	year = {2022},
	month = {may},
	publisher = {IOP Publishing},
	volume = {2},
	number = {2},
	pages = {024008},
	author = {Giulia D’Angelo and Adam Perrett and Massimiliano Iacono and Steve Furber and Chiara Bartolozzi},
	title = {Event driven bio-inspired attentive system for the iCub humanoid robot on SpiNNaker},
	journal = {Neuromorphic Computing and Engineering },
	abstract = {Attention leads the gaze of the observer towards interesting items, allowing a detailed analysis only for selected regions of a scene. A robot can take advantage of the perceptual organisation of the features in the scene to guide its attention to better understand its environment. Current bottom–up attention models work with standard RGB cameras requiring a significant amount of time to detect the most salient item in a frame-based fashion. Event-driven cameras are an innovative technology to asynchronously detect contrast changes in the scene with a high temporal resolution and low latency. We propose a new neuromorphic pipeline exploiting the asynchronous output of the event-driven cameras to generate saliency maps of the scene. In an attempt to further decrease the latency, the neuromorphic attention model is implemented in a spiking neural network on SpiNNaker, a dedicated neuromorphic platform. The proposed implementation has been compared with its bio-inspired GPU counterpart, and it has been benchmarked against ground truth fixational maps. The system successfully detects items in the scene, producing saliency maps comparable with the GPU implementation. The asynchronous pipeline achieves an average of 16 ms latency to produce a usable saliency map.}
}
@article{Preskill:2018,
	  doi = {10.22331/q-2018-08-06-79},
	  url = {https://doi.org/10.22331/q-2018-08-06-79},
	  title = {Quantum {C}omputing in the {NISQ} era and beyond},
	  author = {Preskill, John},
	  journal = {{Quantum}},
	  issn = {2521-327X},
	  publisher = {{Verein zur F{\"{o}}rderung des Open Access Publizierens in den Quantenwissenschaften}},
	  volume = {2},
	  pages = {79},
	  month = aug,
	  year = {2018}
}

@book{National:2019,
	  title={Quantum Computing: Progress and Prospects},
	  author={National Academies of Sciences, E.M. and Sciences, D.E.P. and Board, I.C.S. and Board, C.S.T. and Computing, C.T.A.F.I.Q. and Horowitz, M. and Grumbling, E.},
	  isbn={9780309479691},
	  lccn={2018277625},
	  url={https://books.google.com/books?id=jjiPDwAAQBAJ},
	  year={2019},
	  publisher={National Academies Press}
}
